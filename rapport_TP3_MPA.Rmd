---
title: "rapport_TP3_MPA"
author: "BON, BARON, FOUSSE"
date: "11/10/2020"
output: html_document
---

# Partie 1

## Question 1

La loi générative s'exprime sous la forme :
\[
p(y|\theta) = \prod_{i=1}^{n} p(y_i|\theta) = \prod_{i=1}^n e^{-\theta} \times \frac{\theta^{y_i}}{y_i!} = e^{-n\theta} \times \frac{\theta^{\sum_{i=1}^n y_i}}{\prod_{i=1}^n y_i!} = e^{-n\theta} \times \frac{\theta^{n\overline{y}}}{\prod_{i=1}^ny_i!}
\]

##Question 2

Pour déterminer la loi a posteriori, on utilise la formule de Bayes :

\[
p(\theta|y) \propto p(\theta)p(y|\theta) \propto \frac{1}{\theta}e^{-n\theta}\frac{\theta^{n\overline{y}}}{\prod_{i=1}^ny_i!} \propto e^{-n\theta} \theta^{n\overline{y} - 1}
\]
avec $\overline{y} la moyenne empirique des données.

On peut calculer la constante de normalisation $C$ en intégrant cette expression par rapport à $\theta$.

\[
C = \int_0^{+\infty}e^{-n\theta}\theta^{n\overline{y} - 1}d\theta
\]
En effectuant $n\overline{y}-1$ intégrations par parties, on arrive à :
\[
C = \frac{(n\overline{y} - 1)!}{n^{n\overline{y} - 1}}
\]

Ainsi, finalement on obtient :
\[
p(\theta|y) = \frac{1}{C}e^{-n\theta}\theta^{n\overline{y} - 1}
\]

On peut à présent calculer l'espérance de la loi a posteriori :

\[
\mathbb{E}(\theta|y) = \frac{1}{C}\int_0^{+\infty}\theta p(\theta|y)d\theta = \frac{1}{C}\int_0^{+\infty}e^{-n\theta}\theta^{n\overline{y}}d\theta
\]

En réutilisant le calcul de la constante, on aboutit à :

\[
\mathbb{E}(\theta|y) = \frac{1}{C}\frac{(n\overline{y})!}{n^{n\overline{y} + 1}} = \frac{n\overline{y}}{n} = \overline{y}
\]




```{R}
theta.reel = 11
n = 30 #taille du tirage, ne peut pas être beaucoup plus grand sinon on risque d'overflow dans le calcul de r
y = rpois(n, theta.reel)

#on fait un tirage de thetas avec l'algorithme de Metropolis-Hastings
nthetas = 1000
les_thetas = c()
for (i in seq(1:nthetas)){
  #Metropolis-Hastings
  
  #Etape 1 : initialisation
  niter = 1000
  t = 0
  theta.t = 11

  for (k in seq(1:niter)){
    #Etape 2
    theta.star = rexp(1, rate=1/theta.t)
    #Etape 3
    if ((theta.star / theta.t)^(n*mean(y) - 1) != Inf){
      r = exp(-n * (theta.star - theta.t)) * (theta.star / theta.t)^(n*mean(y) - 1) * dexp(theta.t, rate=1/theta.star) / dexp(theta.star, rate=1/theta.t)
     # print(r)
      if (is.nan(r)){
        next
      }
        #etape 4
      if (1 < r){
        p = 1
      }
      else{
        p = r
      }
    }
    else{
      p = 1
      
    }
    tirage = runif(1)
    if (tirage < p){
      theta.t = theta.star
    }
    #etape 5 done
  }
  les_thetas = c(les_thetas, theta.t)
}



hist(les_thetas, breaks=50, prob=T, main=NULL, xlim=c(0, 30), ylim = c(0,1), col="red")  # on trace l'histogramme des thétas normalisé à 1
x_graph = seq(from=0, to=20, by=0.1)
y_graph = dgamma(x_graph, shape=n*mean(y), scale=1/n) #on trace la densité réelle
lines(x_graph, y_graph) #on superpose les deux graphes

```
L'histogramme correspond. Il serait probablement plus performant avec plus de données, mais cela donnerait des valeurs trop grandes pour êtres calculées par R


# Partie 2

## Question 1:
\[
p(y|z, \theta ) = p(y_1...y_n|z, \theta ) = \prod_{i = 1}^{K}p(y_i|z, \theta )\]
par indépendance des $y_i$.
On obtient : 
\[p(y|z, \theta ) = \prod_{i = 1}^{K}p(y_i|z_i, \theta )\]
Ensuite, on regroupe par rapport aux classes $I_k$ :
\[p(y|z, \theta ) = \prod_{k = 1}^{K}\prod_{i\in I_k}^{}p(y_i|z_i, \theta )\]
Or d'après l'énoncé : 
\[p(y_i|z_i, \theta ) = (\theta_{z_i})^{y_i}\frac{e^{-\theta_{z_i}}}{!y_i}\]
On peut donc remplacer dans la formule, sachant que si $i\in I_k$ $z_i = k$.
D'où : 
\[p(y|z, \theta ) = \prod_{k = 1}^{K}\prod_{i\in I_k}^{}(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}\]


## Question 2:

\[p(z,\theta |y) \propto  p(y| z,\theta)p(\theta )p(z)\]
Or $p(\theta) = p(\theta_1...\theta_K)$ 
On suppose les $\theta_k$ indépendant. De plus $p(z) \propto 1$
D'où :
\[ p(z,\theta |y) \propto \prod_{k = 1}^{K}\frac{1}{\theta _k}\prod_{i\in I_k}^{}(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}\]

## Question 3:

D'après la formule de Bayes:
\[p(z_i|y, \theta ) = \frac{p(y|z_i, \theta )p(z_i|\theta )}{p(y|\theta )}\]
De plus : 
\[p(y|z_i, \theta ) = p(y_i |z_i, \theta )\prod_{j \neq i}^{}p(y_j | \theta )\]
\[ p(y| \theta ) = p(y_i |\theta )\prod_{j \neq i}^{}p(y_j | \theta )\]
\[p(z_i|\theta) = p(z_i) = \frac{1}{K}\]

Alors : 
\[p(z_i|y, \theta ) = \frac{p(y_i|z_i, \theta )}{p(y_i|\theta )K}\]

On applique la formules des probabilités totales :
\[p(y_i|\theta ) = \sum_{k = 1}^{K}p(y_i|z_i = k, \theta )p(zi = k)\] 
$p(z_i = k) = K \forall i,k$ et $p(y_i|z_i, \theta ) = (\theta_{z_i})^{y_i}\frac{e^{-\theta_{z_i}}}{!y_i}$

D'où : 
\[p(z_i = k |y, \theta ) = \frac{(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}}{\sum_{l = 1}^{K}(\theta_{l})^{y_i}\frac{e^{-\theta_{l}}}{!y_i}}\]
Et en simplifiant : 
\[p(z_i = k |y, \theta ) = \frac{1}{\sum_{l = 1}^{K}(\frac {\theta_{l}}{\theta_k})^{y_i}e^{-\theta_{l} + \theta_{k}}}\]

## Question 4:

Comme on connait z, y $\theta_k$ ne dépend que des yi tels que $i \in I_k$ que l'on notera $y_{I_k}$.

On peut donc ecrire : 
\[ p(\theta_k |\theta _{-k}, y,z) = p(\theta_k |\theta _{-k}, y_{I_k},z)\]

Alors :
\[p(\theta_k |\theta _{-k}, y,z) \propto p(y_{I_k} |\theta,z)p(\theta_k)\]

Or $p(\theta_k)=\frac {1}{\theta_k}$.

D'où :

\[p(\theta_k |\theta _{-k}, y,z) \propto \frac{1}{\theta _k}\prod_{i\in I_k}^{}(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}\]

En simplifiant les termes qui ne dépendent pas de $\theta_k$ et en factorisant:

\[p(\theta_k |\theta _{-k}, y,z) \propto (\theta_{k})^{\sum_{i \in I_k}^{}y_i -1}e^{-\sum_{i \in I_k}^{}\theta_{k}}\]

On introduit $n_k$ et en ecrivant $\sum_{i \in I_k}^{}y_i \frac{n_k}{n_k} = n_k\bar{y_k}$:

\[p(\theta_k |\theta _{-k}, y,z) \propto (\theta_{k})^{n_k\bar{y_k} -1}e^{-n_k\theta_{k}}\]










Implémentation de l'algo de Gibbs :

```{R}
# 
# #on commence par récupérer les données
# y <- scan('TP3_mpa_2020.txt')
# n = length(y)
# 
# #on définit le nombre de catégories
# K = 2
# 
# #on initialise les vecteurs z et thetas
# z <- floor(runif(n, 0, K))
# t <- runif(K, 0, max(y))
# 
# #on fait tourner l'algo
# 
# niter=1000
# for (iter in seq(1:niter)){
#   #GS1 : maj de z
#     #On parcourt le vecteur z
#     for (i in seq(1:n)){
#           #On définit la nouvelle loi de probas : vecteur de longueur K des p(z=k)
#          i = 1
#         probas_z = c()
#         for (k in seq (1:K)){
#           probas_z = c(probas_z, t[k]^y[i]*exp(-t[k]) / sum(t^y[i]*exp(-t)))
#         }
#         #print(probas_z)
#         #On fait un tirage suivant cette loi
#         z[i] = sample(1:K, 1, prob=probas_z)
#     }
#   #GS2 : maj de theta
#   #on parcourt le vecteur theta
#     for (k in seq(1:K)){
#       #on recupere et on compte les y dans la catégorie k
#       yk = c()
#       nk = 0
#       for (i in seq(1:n)){
#         if (z[i] == k){
#           yk = c(yk, y[i])
#           nk = nk + 1
#         }
#       }
#       t[k] = rgamma(1, nk * mean(yk), 1 / nk)
#     }
#   
# }
# 
# #maintenant on estime la performance : on calcule la densité de la grande loi en pondérant les poissons
# 
# #on compte la répartitions des zk :
# pk = rep(0, K)
# for (i in seq(1:n)){
#   pk[z[i]] = pk[z[i]] + 1
# }
# #on trace l'histogramme de y :
# hist(y, breaks=50, prob=T, main=NULL, xlim=c(0, max(y)), ylim = c(0,1), col="red")  # on trace l'histogramme des y normalisé à 1
# x_graph = seq(from=0, to=max(y), by=1)
# y_graph = c()
# for (v in seq(1, length(x_graph))){
#   y_graph = c(y_graph, sum(pk * dpois(x_graph[v], t)))
# }
# lines(x_graph, y_graph)



```
