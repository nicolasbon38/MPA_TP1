<<<<<<< HEAD
---
title: "rapport_TP3_MPA"
author: "BON, BARON, FOUSSE"
date: "11/10/2020"
output: html_document
---

# Partie 1

## Question 1

La loi générative s'exprime sous la forme :
\[
p(y|\theta) = \prod_{i=1}^{n} p(y_i|\theta) = \prod_{i=1}^n e^{-\theta} \times \frac{\theta^{y_i}}{y_i!} = e^{-n\theta} \times \frac{\theta^{\sum_{i=1}^n y_i}}{\prod_{i=1}^n y_i!} = e^{-n\theta} \times \frac{\theta^{n\overline{y}}}{\prod_{i=1}^ny_i!}
\]

## Question 2

Pour déterminer la loi a posteriori, on utilise la formule de Bayes :

\[
p(\theta|y) \propto p(\theta)p(y|\theta) \propto \frac{1}{\theta}e^{-n\theta}\frac{\theta^{n\overline{y}}}{\prod_{i=1}^ny_i!} \propto e^{-n\theta} \theta^{n\overline{y} - 1}
\]
avec $\overline{y} la moyenne empirique des données.

On peut calculer la constante de normalisation $C$ en intégrant cette expression par rapport à $\theta$.

\[
C = \int_0^{+\infty}e^{-n\theta}\theta^{n\overline{y} - 1}d\theta
\]
En effectuant $n\overline{y}-1$ intégrations par parties, on arrive à :
\[
C = \frac{(n\overline{y} - 1)!}{n^{n\overline{y} - 1}}
\]

Ainsi, finalement on obtient :
\[
p(\theta|y) = \frac{1}{C}e^{-n\theta}\theta^{n\overline{y} - 1}
\]

On peut à présent calculer l'espérance de la loi a posteriori :

\[
\mathbb{E}(\theta|y) = \frac{1}{C}\int_0^{+\infty}\theta p(\theta|y)d\theta = \frac{1}{C}\int_0^{+\infty}e^{-n\theta}\theta^{n\overline{y}}d\theta
\]

En réutilisant le calcul de la constante, on aboutit à :

\[
\mathbb{E}(\theta|y) = \frac{1}{C}\frac{(n\overline{y})!}{n^{n\overline{y} + 1}} = \frac{n\overline{y}}{n} = \overline{y}
\]


## Question 3

```{R}
theta.reel = 11
n = 30#taille du tirage, ne peut pas être beaucoup plus grand sinon on risque d'overflow dans le calcul de r
y = rpois(n, theta.reel)

#on fait un tirage de thetas avec l'algorithme de Metropolis-Hastings
nthetas = 1000
les_thetas = c()
for (i in seq(1:nthetas)){
  #Metropolis-Hastings
  
  #Etape 1 : initialisation
  niter = 100
  theta.t = 11

  for (k in seq(1:niter)){
    #Etape 2
    theta.star = rexp(1, rate=1/theta.t)
    #Etape 3
    if ((theta.star / theta.t)^(n*mean(y) - 1) != Inf){
      r = exp(-n * (theta.star - theta.t)) * (theta.star / theta.t)^(n*mean(y) - 1) * dexp(theta.star, rate=1/theta.t) / dexp(theta.t, rate=1/theta.star)
     # print(r)
      if (is.nan(r)){
        next
      }
        #etape 4
      if (1 < r){
        p = 1
      }
      else{
        p = r
      }
    }
    else{
      p = 1
      
    }
    tirage = runif(1)
    if (tirage < p){
      theta.t = theta.star
    }
    #etape 5 done
  }
  les_thetas = c(les_thetas, theta.t)
}
```

## Question 4 
On obtient la moyenne, la médiane et l'intervalle de confaince par les commandes suivantes

```{R}
value = les_thetas
summary(value)
t.test(value)

```



## Question 5 
On trace l'histogramme des theta obtenus à la qu 3, en superposant la loi trouvée à la question 2.
```{R}
hist(les_thetas, breaks=20, prob=T, main=NULL, xlim=c(0, 30), ylim = c(0,1), col="green")  # on trace l'histogramme des thétas normalisé à 1
x_graph = seq(from=0, to=20, by=0.1)
y_graph = dgamma(x_graph, shape=n*mean(y), scale= 1/n) #on trace la densité réelle
lines(x_graph, y_graph, col = "red") #on superpose les deux graphes

```
L'histogramme correspond. Il serait probablement plus performant avec plus de données, mais cela donnerait des valeurs trop grandes pour êtres calculées par R

## Question 6

\[p(\tilde{y}|y) = p(\tilde{y}|\theta) = e^{-\theta} \times \frac{\theta^{\tilde{y}}}{\tilde{y}!}\]
On reprend donc le même procédé qu'a la question deux, en choisissant le $\theta$ le plus probable qui correspond ici à la moyenne.

```{R}
theta = 11
n = 30 #taille du tirage, ne peut pas être beaucoup plus grand sinon on risque d'overflow dans le calcul de r

#on fait un tirage de thetas avec l'algorithme de Metropolis-Hastings
ny = 100
les_y = c()
for (i in seq(1:ny)){
  #Metropolis-Hastings
  
  #Etape 1 : initialisation
  niter = 100
  y.t = 11

  for (k in seq(1:niter)){
    #Etape 2
    y.star = rexp(1, rate=1/y.t)
    #Etape 3
    if (1 != Inf){
      r = (theta)^(y.star - y.t) * (factorial(y.t)/factorial(y.star))* dexp(y.t, rate=1/y.star) / dexp(y.star, rate=1/y.t)
     # print(r)
      if (is.nan(r)){
        next
      }
        #etape 4
      if (1 < r){
        p = 1
      }
      else{
        p = r
      }
    }
    else{
      p = 1
      
    }
    tirage = runif(1)
    if (tirage < p){
      y.t = y.star
    }
    #etape 5 done
  }
  les_y = c(les_y, y.t)
}

hist(les_y, breaks=20, prob=T, main=NULL, xlim=c(0, 30), ylim = c(0,1), col="green")  # on trace l'histogramme des y normalisé à 1
x_graph = seq(from=0, to=20, by=1)
y_graph = dpois(x_graph, lambda = theta) #on trace la densité réelle
lines(x_graph, y_graph, col = "red") #on su

hist(y, breaks=20, prob=T, main=NULL, xlim=c(0, 30), ylim = c(0,1), col="red")

```
Il est difficle de comparer les valeurs calculées aux valeurs de départ car il y a peu de valeurs de départs (à cause de la capacité de calculs limitées). Cependant, on peut comparer l'histogramme des y calculés avec la densité théorique. Les y calculés sont legèrement decalés par rapport à la densité theorique (à cause de l'incertitude du paramètre $\theta$) mais ont tout de même la même forme.




# Partie 2

## Question 1:
\[
p(y|z, \theta ) = p(y_1...y_n|z, \theta ) = \prod_{i = 1}^{K}p(y_i|z, \theta )\]
par indépendance des $y_i$.
On obtient : 
\[p(y|z, \theta ) = \prod_{i = 1}^{K}p(y_i|z_i, \theta )\]
Ensuite, on regroupe par rapport aux classes $I_k$ :
\[p(y|z, \theta ) = \prod_{k = 1}^{K}\prod_{i\in I_k}^{}p(y_i|z_i, \theta )\]
Or d'après l'énoncé : 
\[p(y_i|z_i, \theta ) = (\theta_{z_i})^{y_i}\frac{e^{-\theta_{z_i}}}{!y_i}\]
On peut donc remplacer dans la formule, sachant que si $i\in I_k$ $z_i = k$.
D'où : 
\[p(y|z, \theta ) = \prod_{k = 1}^{K}\prod_{i\in I_k}^{}(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}\]


## Question 2:

\[p(z,\theta |y) \propto  p(y| z,\theta)p(\theta )p(z)\]
Or $p(\theta) = p(\theta_1...\theta_K)$ 
On suppose les $\theta_k$ indépendant. De plus $p(z) \propto 1$
D'où :
\[ p(z,\theta |y) \propto \prod_{k = 1}^{K}\frac{1}{\theta _k}\prod_{i\in I_k}^{}(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}\]

## Question 3:

D'après la formule de Bayes:
\[p(z_i|y, \theta ) = \frac{p(y|z_i, \theta )p(z_i|\theta )}{p(y|\theta )}\]
De plus : 
\[p(y|z_i, \theta ) = p(y_i |z_i, \theta )\prod_{j \neq i}^{}p(y_j | \theta )\]
\[ p(y| \theta ) = p(y_i |\theta )\prod_{j \neq i}^{}p(y_j | \theta )\]
\[p(z_i|\theta) = p(z_i) = \frac{1}{K}\]

Alors : 
\[p(z_i|y, \theta ) = \frac{p(y_i|z_i, \theta )}{p(y_i|\theta )K}\]

On applique la formules des probabilités totales :
\[p(y_i|\theta ) = \sum_{k = 1}^{K}p(y_i|z_i = k, \theta )p(zi = k)\] 
$p(z_i = k) = K \forall i,k$ et $p(y_i|z_i, \theta ) = (\theta_{z_i})^{y_i}\frac{e^{-\theta_{z_i}}}{!y_i}$

D'où : 
\[p(z_i = k |y, \theta ) = \frac{(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}}{\sum_{l = 1}^{K}(\theta_{l})^{y_i}\frac{e^{-\theta_{l}}}{!y_i}}\]
Et en simplifiant : 
\[p(z_i = k |y, \theta ) = \frac{1}{\sum_{l = 1}^{K}(\frac {\theta_{l}}{\theta_k})^{y_i}e^{-\theta_{l} + \theta_{k}}}\]

## Question 4:

Comme on connait z, y $\theta_k$ ne dépend que des yi tels que $i \in I_k$ que l'on notera $y_{I_k}$.

On peut donc ecrire : 
\[ p(\theta_k |\theta _{-k}, y,z) = p(\theta_k |\theta _{-k}, y_{I_k},z)\]

Alors :
\[p(\theta_k |\theta _{-k}, y,z) \propto p(y_{I_k} |\theta,z)p(\theta_k)\]

Or $p(\theta_k)=\frac {1}{\theta_k}$.

D'où :

\[p(\theta_k |\theta _{-k}, y,z) \propto \frac{1}{\theta _k}\prod_{i\in I_k}^{}(\theta_{k})^{y_i}\frac{e^{-\theta_{k}}}{!y_i}\]

En simplifiant les termes qui ne dépendent pas de $\theta_k$ et en factorisant:

\[p(\theta_k |\theta _{-k}, y,z) \propto (\theta_{k})^{\sum_{i \in I_k}^{}y_i -1}e^{-\sum_{i \in I_k}^{}\theta_{k}}\]

On introduit $n_k$ et en ecrivant $\sum_{i \in I_k}^{}y_i \frac{n_k}{n_k} = n_k\bar{y_k}$:

\[p(\theta_k |\theta _{-k}, y,z) \propto (\theta_{k})^{n_k\bar{y_k} -1}e^{-n_k\theta_{k}}\]




## Question 5 


Implémentation de l'algo de Gibbs, et test avec $K = 2$, $\theta_1 =  20$, $\theta_2 = 100$, $p_1 = \frac{1}{3}$, $p_2 = \frac{2}{3}$:

```{R}

#on commence par récupérer les données
#y <- scan('TP3_mpa_2020.txt')
theta1 = 20
theta2 = 100
y = c(rpois(100, theta1), rpois(200, theta2))
n = length(y)

#on définit le nombre de catégories
K = 3

#on initialise les vecteurs z et thetas
z <- floor(runif(n, 1, K+1)) # il n'y a que deux catégories alors les valeur seront 1 et 2
#t <- runif(K, 0, max(y)) #theta init entre 0 et max y
t<- rep(mean(y), K)
t
#on fait tourner l'algo

niter=1000
for (iter in seq(1:niter)){
  #GS1 : maj de z
    #On parcourt le vecteur z
    for (i in seq(1:n)){
          #On définit la nouvelle loi de probas : vecteur de longueur K des p(z=k)
        probas_z = c()
        for (k in seq (1:K)){
          new =  sum((t/t[k])^y[i]*exp(-t + t[k]))
          probas_z = c(probas_z, 1 /new)
        }
        #print(probas_z)
        #On fait un tirage suivant cette loi
        
        z[i] = sample(1:K, 1, prob=probas_z)
    }
  #GS2 : maj de theta
  #on parcourt le vecteur theta
    for (k in seq(1:K)){
      #on recupere et on compte les y dans la catégorie k
      yk = c()
      nk = 0
      for (i in seq(1:n)){
        if (z[i] == k){
          yk = c(yk, y[i])
          nk = nk + 1
        }
      }
      t[k] = rgamma(1, nk * mean(yk), nk)
    }
}

print(t)
#maintenant on estime la performance : on calcule la densité de la grande loi en pondérant les poissons

#on compte la répartitions des zk :
pk = rep(0, K)
for (i in seq(1:n)){
  pk[z[i]] = pk[z[i]] + 1
}
pk = pk/length(z)
#on trace l'histogramme de y :
hist(y, breaks=50, prob=T, main=NULL, xlim=c(0, max(y)), ylim = c(0,0.2), col="red")  # on trace l'histogramme des y normalisé à 1
x_graph = seq(from=0, to=max(y), by=1)
y_graph = c()
for (v in seq(1, length(x_graph))){
  y_graph = c(y_graph, sum(pk * dpois(x_graph[v], t)))
}
lines(x_graph, y_graph)



```

L'algorithme est assez performant.
On peut obtenir $\mathbb{E}(\theta_k|y)$ en prenant les velaurs moyennes de chaque classe, et on retrouve les $p_k$ en étudiant la fréquence de chaque $z_k$ dans le vecteur $z$.

## Question 7

On applique l'algorithe sur les données de l'énoncé :


```{R}


Gibbs.a.K.classes <- function(K, printgraphs){
  #on commence par récupérer les données
  y <- scan('TP3_mpa_2020.txt')
  
  n = length(y)
  
   #on contruit le barplot stylé du cours avec une matrix
  matrice.probas_z = matrix(nrow=K, ncol=length(y))
  
  #on définit le nombre de catégories
  
  #on initialise les vecteurs z et thetas
  z <- floor(runif(n, 1, K+1)) # il n'y a que deux catégories alors les valeur seront 1 et 2
  #t <- runif(K, 0, max(y)) #theta init entre 0 et max y
  t<- rep(mean(y), K)
  t
  #on fait tourner l'algo
  
  niter=1000
  for (iter in seq(1:niter)){
    #GS1 : maj de z
      #On parcourt le vecteur z
      for (i in seq(1:n)){
            #On définit la nouvelle loi de probas : vecteur de longueur K des p(z=k)
          probas_z = c()
          for (k in seq (1:K)){
            new =  sum((t/t[k])^y[i]*exp(-t + t[k]))
            probas_z = c(probas_z, 1 /new)
            matrice.probas_z[k, i] = 1/new
          }
          #print(probas_z)
          #On fait un tirage suivant cette loi
          
          z[i] = sample(1:K, 1, prob=probas_z)
      }
    #GS2 : maj de theta
    #on parcourt le vecteur theta
      for (k in seq(1:K)){
        #on recupere et on compte les y dans la catégorie k
        yk = c()
        nk = 0
        for (i in seq(1:n)){
          if (z[i] == k){
            yk = c(yk, y[i])
            nk = nk + 1
          }
        }
        t[k] = rgamma(1, nk * mean(yk), nk)
      }
  }
  
  #maintenant on estime la performance : on calcule la densité de la grande loi en pondérant les poissons
  
  #on compte la répartitions des zk :
  pk = rep(0, K)
  for (i in seq(1:n)){
    pk[z[i]] = pk[z[i]] + 1
  }
  pk = pk/length(z)
  if (printgraphs){
    #on trace l'histogramme de y :
    hist(y, breaks=50, prob=T, main=NULL, xlim=c(0, max(y)), ylim = c(0,0.2), col="red")  # on trace l'histogramme des y normalisé à 1
    x_graph = seq(from=0, to=max(y), by=1)
    y_graph = c()
    for (v in seq(1, length(x_graph))){
      y_graph = c(y_graph, sum(pk * dpois(x_graph[v], t)))
    }
    lines(x_graph, y_graph)
    colors=c("blue", "red", "yellow", "green", "pink")
    barplot(matrice.probas_z, beside=FALSE, horiz=FALSE, xlim=c(0, 210), ylim=c(0, 3), col=  colors[1:K], main="proba d'appartenir à chaque classe pour chaque valeur")
  }
  return(list(pk, t))
}

# 
Gibbs.a.K.classes(2, TRUE)
Gibbs.a.K.classes(3, TRUE)
Gibbs.a.K.classes(4, TRUE)
Gibbs.a.K.classes(5, TRUE)

```

La densité obtenue avec 2 classes ne correspond pas du tout à l'histogramme, on peut la rejeter.

Pour 4 classes, on voit que la classe bleue et la classe jaune ne sont pas vraiment discriminées par l'algorithme. Pour 5 classes ce sont les classes bleue, rouge et jaune qui ne sont pas détectées.

Ainsi le modèle à trois classes paraît être le plus adapté pour décrire les données.

On  peut le vérifier en tirant un nouveau set de données $\tilde{y}$ avec la loi ainsi déterminée, et en étudiant différents paramètres statistiues tels que la skewness et la kurtosis.

```{R}
#on récupère les pk et les t
for (K in seq(from=2, to=5, by=1)){
  a = Gibbs.a.K.classes(K, FALSE)
  pk = a[1]
  t = a[2]
  t = unlist(t)
  
  #on tire 200 nouveau y :
  y.tilde = c()
  for (i in seq(1:500)){
    idx.poisson = sample(1:K, 1, prob=unlist(pk))
    y.tilde = c(y.tilde, rpois(1, t[idx.poisson]))
  }
  
  #on compare les skewness :
  library(moments)
  y <- scan('TP3_mpa_2020.txt')
  print(K)
  print(skewness(y.tilde))
  
  
  print(skewness(y))

  print(kurtosis(y))
  print(kurtosis(y.tilde))
}

```

On voit dans ce calcul que les paramètres statistiques collent mieux à la modélisation par 3, 4 et 5 classes.


Mais, on voit sur les diagrammes en barres que les modélisations à 4 et 5 classes ne sont pas adaptées comme on l'a vu dans le paragraphe précédent.

Ainsi, on gardera la modélisation à 3 classes, avec :

$\theta_1 = 2.57 et p_1 = 0.24$
$\theta_2 = 10.86 et p_2 = 0.47$
$\theta_3 = 19.3 et p_3 = 0.28$




