---
title: "rapport_TP3_MPA"
author: "BON, BARON, FOUSSE"
date: "11/10/2020"
output: html_document
---

# Partie 1

## Question 1

La loi générative s'exprime sous la forme :
\[
p(y|\theta) = \prod_{i=1}^{n} p(y_i|\theta) = \prod_{i=1}^n e^{-\theta} \times \frac{\theta^{y_i}}{y_i!} = e^{-n\theta} \times \frac{\theta^{\sum_{i=1}^n y_i}}{\prod_{i=1}^n y_i!} = e^{-n\theta} \times \frac{\theta^{n\overline{y}}}{\prod_{i=1}^ny_i!}
\]

##Question 2

Pour déterminer la loi a posteriori, on utilise la formule de Bayes :

\[
p(\theta|y) \propto p(\theta)p(y|\theta) \propto \frac{1}{\theta}e^{-n\theta}\frac{\theta^{n\overline{y}}}{\prod_{i=1}^ny_i!} \propto e^{-n\theta} \theta^{n\overline{y} - 1}
\]
avec $\overline{y} la moyenne empirique des données.

On peut calculer la constante de normalisation $C$ en intégrant cette expression par rapport à $\theta$.

\[
C = \int_0^{+\infty}e^{-n\theta}\theta^{n\overline{y} - 1}d\theta
\]
En effectuant $n\overline{y}-1$ intégrations par parties, on arrive à :
\[
C = \frac{(n\overline{y} - 1)!}{n^{n\overline{y} - 1}}
\]

Ainsi, finalement on obtient :
\[
p(\theta|y) = \frac{1}{C}e^{-n\theta}\theta^{n\overline{y} - 1}
\]

On peut à présent calculer l'espérance de la loi a posteriori :

\[
\mathbb{E}(\theta|y) = \frac{1}{C}\int_0^{+\infty}\theta p(\theta|y)d\theta = \frac{1}{C}\int_0^{+\infty}e^{-n\theta}\theta^{n\overline{y}}d\theta
\]

En réutilisant le calcul de la constante, on aboutit à :

\[
\mathbb{E}(\theta|y) = \frac{1}{C}\frac{(n\overline{y})!}{n^{n\overline{y} + 1}} = \frac{n\overline{y}}{n} = \overline{y}
\]




```{R}
theta.reel = 11
n = 30 #taille du tirage, ne peut pas être beaucoup plus grand sinon on risque d'overflow dans le calcul de r
y = rpois(n, theta.reel)

#on fait un tirage de thetas avec l'algorithme de Metropolis-Hastings
nthetas = 1000
les_thetas = c()
for (i in seq(1:nthetas)){
  #Metropolis-Hastings
  
  #Etape 1 : initialisation
  niter = 1000
  t = 0
  theta.t = 11

  for (k in seq(1:niter)){
    #Etape 2
    theta.star = rexp(1, rate=1/theta.t)
    #Etape 3
    if ((theta.star / theta.t)^(n*mean(y) - 1) != Inf){
      r = exp(-n * (theta.star - theta.t)) * (theta.star / theta.t)^(n*mean(y) - 1) * dexp(theta.t, rate=1/theta.star) / dexp(theta.star, rate=1/theta.t)
     # print(r)
      if (is.nan(r)){
        next
      }
        #etape 4
      if (1 < r){
        p = 1
      }
      else{
        p = r
      }
    }
    else{
      p = 1
      
    }
    tirage = runif(1)
    if (tirage < p){
      theta.t = theta.star
    }
    #etape 5 done
  }
  les_thetas = c(les_thetas, theta.t)
}



print(length(les_thetas) - length(les_thetas_filtres))
hist(les_thetas, breaks=50, prob=T, main=NULL, xlim=c(0, 30), ylim = c(0,1), col="red")  # on trace l'histogramme des thétas normalisé à 1
x_graph = seq(from=0, to=20, by=0.1)
y_graph = dgamma(x_graph, shape=n*mean(y), scale=1/n) #on trace la densité réelle
lines(x_graph, y_graph) #on superpose les deux graphes

```
L'histogramme correspond. Il serait probablement plus performant avec plus de données, mais cela donnerait des valeurs trop grandes pour êtres calculées par R


# Partie 2

Là il faut rédiger la partie théorique



Implémentation de l'algo de Gibbs :

```{R}
#on commence par récupérer les données
y <- scan('TP3_mpa_2020.txt')
n = length(y)

#on définit le nombre de catégories
K = 2

#on initialise les vecteurs z et thetas
z <- floor(runif(n, 0, K))
t <- runif(K, 0, max(y))

#on fait tourner l'algo

niter=100
for (iter in seq(1:niter)){
  #GS1 : maj de z
    #On parcourt le vecteur z
    for (i in seq(1:n)){
          #On définit la nouvelle loi de probas : vecteur de longueur K des p(z=k)
         i = 1
        probas_z = c()
        for (k in seq (1:K)){
          probas_z = c(probas_z, t[k]^y[i]*exp(-t[k]) / sum(t^y[i]*exp(-t)))
        }
        #print(probas_z)
        #On fait un tirage suivant cette loi
        z[i] = sample(1:K, 1, prob=probas_z)
    }
  #GS2 : maj de theta
  #on parcourt le vecteur theta
    for (k in seq(1:K)){
      #on recupere et on compte les y dans la catégorie k
      yk = c()
      nk = 0
      for (i in seq(1:n)){
        if (z[i] == k){
          yk = c(yk, y[i])
          nk = nk + 1
        }
      }
      t[k] = rgamma(1, nk * mean(yk), 1 / nk)
    }
  
}

#maintenant on estime la performance : on calcule la densité de la grande loi en pondérant les poissons

#on compte la répartitions des zk :
pk = rep(0, K)
for (i in seq(1:n)){
  pk[z[i]] = pk[z[i]] + 1
}
#on trace l'histogramme de y :
hist(y, breaks=50, prob=T, main=NULL, xlim=c(0, max(y)), ylim = c(0,1), col="red")  # on trace l'histogramme des y normalisé à 1
x_graph = seq(from=0, to=max(y), by=1)
y_graph = c()
for (v in seq(1, length(x_graph))){
  y_graph = c(y_graph, sum(pk * dpois(x_graph[v], t)))
}
lines(x_graph, y_graph)


```
